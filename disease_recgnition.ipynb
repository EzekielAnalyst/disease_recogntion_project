{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, accuracy_score, precision_score, recall_score, f1_score, precision_recall_curve, average_precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import warnings\n",
    "from itertools import cycle\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'Data/dataset.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Initialize an empty list to store disease and symptom pairs\n",
    "disease_symptom_pairs = []\n",
    "\n",
    "# Extract diseases and symptoms from each row\n",
    "for row in df.itertuples(index=False):\n",
    "    disease = row[0]\n",
    "    symptoms = [symptom for symptom in row[1:] if pd.notna(symptom)]\n",
    "    disease_symptom_pairs.append((disease, symptoms))\n",
    "\n",
    "# Create a list of all unique symptoms\n",
    "all_symptoms = set()\n",
    "for _, symptoms in disease_symptom_pairs:\n",
    "    all_symptoms.update(symptoms)\n",
    "\n",
    "# Create a DataFrame with a row for each occurrence of a disease and columns for each symptom\n",
    "binary_df = pd.DataFrame(columns=['prognosis'] + list(all_symptoms))\n",
    "\n",
    "# Populate the DataFrame\n",
    "for disease, symptoms in disease_symptom_pairs:\n",
    "    row = {symptom: 1 if symptom in symptoms else 0 for symptom in all_symptoms}\n",
    "    row['prognosis'] = disease\n",
    "    binary_df = pd.concat([binary_df, pd.DataFrame([row])], ignore_index=True)\n",
    "\n",
    "# Reorder columns to move 'Prognosis' to the end\n",
    "columns = binary_df.columns.tolist()\n",
    "columns.append(columns.pop(columns.index('prognosis')))\n",
    "binary_df = binary_df[columns]\n",
    "\n",
    "\n",
    "\n",
    "# Save the transformed DataFrame to a new CSV file (optional)\n",
    "output_path = 'Training.csv'\n",
    "binary_df.to_csv(output_path, index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load the datasets\n",
    "training = pd.read_csv('Training.csv')\n",
    "# testing = pd.read_csv('Data/Testing.csv')\n",
    "\n",
    "# Extract features and target variable\n",
    "cols = training.columns[:-1]\n",
    "x = training[cols]\n",
    "y = training['prognosis']\n",
    "\n",
    "# Aggregate data by disease\n",
    "reduced_data = training.groupby(training['prognosis']).max()\n",
    "\n",
    "# Encode the target variable\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(y)\n",
    "y = le.transform(y)\n",
    "\n",
    "# Binarize the output for multiclass evaluation\n",
    "y_bin = label_binarize(y, classes=np.arange(len(le.classes_)))\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=42)\n",
    "y_test_bin = label_binarize(y_test, classes=np.arange(len(le.classes_)))\n",
    "\n",
    "# Train a Decision Tree model\n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "# Cross-validation score\n",
    "scores = cross_val_score(clf, x, y, cv=3)\n",
    "print(\"Decision Tree Cross-validation mean score:\", scores.mean())\n",
    "\n",
    "# Predictions\n",
    "y_pred_tree = clf.predict(x_test)\n",
    "y_score_tree = clf.predict_proba(x_test)\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred_tree)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# Precision\n",
    "precision = precision_score(y_test, y_pred_tree, average='weighted')\n",
    "print(f\"Precision: {precision}\")\n",
    "\n",
    "# Recall\n",
    "recall = recall_score(y_test, y_pred_tree, average='weighted')\n",
    "print(f\"Recall: {recall}\")\n",
    "\n",
    "# F1 Score\n",
    "f1 = f1_score(y_test, y_pred_tree, average='weighted')\n",
    "print(f\"F1 Score: {f1}\")\n",
    "\n",
    "# Classification Report\n",
    "print(\"Decision Tree Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_tree))\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix_tree = confusion_matrix(y_test, y_pred_tree)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(conf_matrix_tree, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Decision Tree Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Compute macro-average ROC curve and ROC area\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "n_classes = y_bin.shape[1]\n",
    "\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_score_tree[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test_bin.ravel(), y_score_tree.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "# Compute macro-average ROC curve and ROC area\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "mean_tpr /= n_classes\n",
    "\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "         label='micro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"micro\"]),\n",
    "         color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "         label='macro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"macro\"]),\n",
    "         color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve for Multiclass')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Compute Precision-Recall curve for each class and average precision\n",
    "precision = dict()\n",
    "recall = dict()\n",
    "average_precision = dict()\n",
    "\n",
    "for i in range(n_classes):\n",
    "    precision[i], recall[i], _ = precision_recall_curve(y_test_bin[:, i], y_score_tree[:, i])\n",
    "    average_precision[i] = average_precision_score(y_test_bin[:, i], y_score_tree[:, i])\n",
    "\n",
    "# Compute micro-average Precision-Recall curve and area\n",
    "precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(y_test_bin.ravel(), y_score_tree.ravel())\n",
    "average_precision[\"micro\"] = average_precision_score(y_test_bin, y_score_tree, average=\"micro\")\n",
    "\n",
    "# Compute macro-average Precision-Recall curve and area\n",
    "all_recall = np.unique(np.concatenate([recall[i] for i in range(n_classes)]))\n",
    "mean_precision = np.zeros_like(all_recall)\n",
    "\n",
    "for i in range(n_classes):\n",
    "    mean_precision += np.interp(all_recall, recall[i], precision[i])\n",
    "\n",
    "mean_precision /= n_classes\n",
    "\n",
    "precision[\"macro\"] = mean_precision\n",
    "recall[\"macro\"] = all_recall\n",
    "average_precision[\"macro\"] = average_precision_score(y_test_bin, y_score_tree, average=\"macro\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(recall[\"micro\"], precision[\"micro\"], color='gold', lw=2,\n",
    "         label='micro-average Precision-Recall curve (area = {0:0.2f})'\n",
    "               ''.format(average_precision[\"micro\"]))\n",
    "\n",
    "plt.plot(recall[\"macro\"], precision[\"macro\"], color='navy', linestyle=':', linewidth=4,\n",
    "         label='macro-average Precision-Recall curve (area = {0:0.2f})'\n",
    "               ''.format(average_precision[\"macro\"]))\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve for Multiclass')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Save the model\n",
    "with open('decision_tree_model.pkl', 'wb') as file:\n",
    "    pickle.dump(clf, file)\n",
    "print(\"Model saved to decision_tree_model.pkl\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
